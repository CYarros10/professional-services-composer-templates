# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Before configuring YAML and generate DAG, please setup MySQL connection by using Airflow webserver as mentioned:
# https://airflow.apache.org/docs/apache-airflow/1.10.14/howto/connection/mysql.html

# To configure follow the steps mentioned below
# 1. Go to Airflow Webserver
# 2. Go to connections
# 3. Click on + and select MySQL
# 4. Add all the required host, port and other connection details.

# To Export data to GCS using this DAG tmplate please configure the Service account with apporiate roles and access as mentioned:
# https://cloud.google.com/sql/docs/mysql/import-export/import-export-csv#export_data_from

# DAG parameters
# mandatory
dag_id: cloudsql_tasks_dag     
description:
max_active_runs:
catchup: False
# mandatory; enclose values within quotes
schedule_interval: "None"        
tags: ["test"]
owner:

# mandatory
default_args:
    owner: 'test'
    retries:
    email_on_failure: False
    email_on_retry: False
    # ex - [test@test.com]
    email:                          
    # minutes
    retry_delay: 1                  

#Optional
#Python functions used by Python operator
functions:
  # perform_transformation:
  #   description: Sample function as an example to perform custom transformation.
  #   code: |
  #     def transformation(data):
  #       """
  #       Sample function as an example to perform custom transformation.

  #       Args:
  #         data: Sample data on which we can perform any transformation.
        
  #       Returns:
  #         The data converted into a string format.
  #       """
  #       print("Printing sample payload from transformation function: {}".format(data))
  #       output = str(data)
  #       return output
  # pull_xcom:
  #   description: Function to pull xcom variables from export GCS task print or use it for other transformations.
  #   code: |
  #     def pull_xcom(**kwargs):
  #       """
  #       Pulls a value from XCom and prints it.
  #       """
  #       ti = kwargs['ti']
  #       pulled_value = str(ti.xcom_pull(task_ids='export_sales_reporting_table_to_gcs', key='file_details'))
  #       print(f"Pulled value from XCom: {pulled_value}")
  #       return pulled_value

#Environment specific configs
# mandatory
envs:
    # mandatory
    default:
        # DAG tasks configs
        # mandatory
        tasks:
        #cc_operator_description: Performs DDL or DML SQL queries in Google Cloud SQL instance.
        - task_id: cloud_sql_truncate_sales_table_task
          task_type: airflow.providers.google.cloud.operators.cloud_sql.CloudSQLExecuteQueryOperator
          gcp_cloudsql_conn_id: cc_var_gcp_cloudsql_conn_id
          sql: cc_var_truncate_sales_table_sql
          trigger_rule : 'all_done'
          upstream_task: None
        #cc_operator_description: Import data into a Cloud SQL instance from Cloud Storage.
        - task_id: cloud_sql_import_sales_data_from_gcs
          task_type: airflow.providers.google.cloud.operators.cloud_sql.CloudSQLImportInstanceOperator
          instance: cc_var_composer_instance
          body: cc_var_import_from_gcs_cloud_sql_body
          trigger_rule : 'all_done'
          upstream_task: cloud_sql_truncate_sales_table_task
        #cc_operator_description: Performs DDL or DML SQL queries in Google Cloud SQL instance.
        - task_id: cloud_sql_drop_sales_reporting_table_task
          task_type: airflow.providers.google.cloud.operators.cloud_sql.CloudSQLExecuteQueryOperator
          gcp_cloudsql_conn_id: cc_var_gcp_cloudsql_conn_id
          sql: cc_var_drop_sales_reporting_table_sql
          trigger_rule : 'all_done'
          upstream_task: cloud_sql_import_sales_data_from_gcs
        #cc_operator_description: Performs DDL or DML SQL queries in Google Cloud SQL instance.
        - task_id: cloud_sql_create_sales_reporting_table_task
          task_type: airflow.providers.google.cloud.operators.cloud_sql.CloudSQLExecuteQueryOperator
          gcp_cloudsql_conn_id: cc_var_gcp_cloudsql_conn_id
          sql: cc_var_create_sales_table_sql
          trigger_rule : 'all_done'
          upstream_task: cloud_sql_drop_sales_reporting_table_task
        #cc_operator_description: Export data from a Cloud SQL instance to a Cloud Storage bucket.
        - task_id: export_sales_reporting_table_to_gcs
          task_type: airflow.providers.google.cloud.operators.cloud_sql.CloudSQLExportInstanceOperator
          instance: cc_var_composer_instance
          body: cc_var_export_to_gcs_cloud_sql_body
          trigger_rule : 'all_done'
          upstream_task: cloud_sql_create_sales_reporting_table_task
        #cc_operator_description: print the GCS uri where the file is written.
        # - task_id: print_gcs_uri
        #   task_type: airflow.operators.python_operator.PythonOperator
        #   python_callable: pull_xcom
        #   trigger_rule : 'all_done'
        #   upstream_task: export_sales_reporting_table_to_gcs
    composer-templates-dev:
        # Below connection_id is retrieved from Google Cloud Secret Manager. 
        # A secret should be created with the naming convention: airflow-connections-<variable_name> ex: airflow-connections-airflow_composer_template_mysql
        cc_var_gcp_cloudsql_conn_id : airflow_composer_template_mysql
        cc_var_truncate_sales_table_sql : 'TRUNCATE TABLE sales;'
        cc_var_drop_sales_reporting_table_sql: 'DROP TABLE IF EXISTS sales_reporting;'
        cc_var_create_sales_table_sql: 'create table sales_reporting as select s.sale_id, p.product_name, p.description, s.order_date, s.city, s.state, p.price as product_price, s.quantity, (s.quantity*p.price) as actual_sell_price, s.total_price as sell_price, s.total_price - (s.quantity*p.price) as difference from sales s inner join products p on s.product_id = p.product_id;'
        cc_var_composer_instance: composer-template
        cc_var_import_from_gcs_cloud_sql_body: {"importContext": {"fileType": "CSV", "uri": "gs://hmh_composer_demo/demo_test_csv/daily_sales.csv", "database": "transactions", "csvImportOptions": {"table": "sales","escapeCharacter":  "5C", "quoteCharacter": "22", "fieldsTerminatedBy": "2C", "linesTerminatedBy": "0A"}}}
        cc_var_export_to_gcs_cloud_sql_body: {"exportContext":{"fileType": "CSV","uri": "gs://hmh_composer_demo/demo_test_csv/sales_report.csv", "databases": ["transactions"], "offload": True, "csvExportOptions": {"selectQuery": "SELECT * FROM sales_reporting;"}}}
